{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project thesis xgBoost\n",
    "\n",
    "This is a notebook for testing package xgBoost (gradient boosting) with hyperparameteroptimization using optuna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries to be used in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy import stats\n",
    "import shap\n",
    "import random\n",
    "import pyarrow.feather as feather\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load morph and SNP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#180K:\n",
    "#data = pd.read_feather(\"C:/Users/gard_/Documents/MasterThesis/ProjectThesis/MyPipeline/Data/Processed/massBV.feather\")\n",
    "\n",
    "#70k:\n",
    "data = pd.read_feather(\"C:/Users/gard_/Documents/MasterThesis/ProjectThesis/MyPipeline/Data/Processed/massBV_70k.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# See that there are no repeated measurements for individuals\n",
    "print(data[\"ringnr\"].duplicated().any())\n",
    "#print(massMorph[\"ringnr\"].duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we load the CV-folds created in 'CreateCV', if we want to use those. The ringnrs are saved in a CSV-file 'cv_folds.csv'.\n",
    "\n",
    "We want to create 10 sets of hyperparameters to be used in xgBoost 10-fold CV. See figure in RM for visualization. \"Nested CV\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('C:/Users/gard_/Documents/MasterThesis/ProjectThesis/MyPipeline/Data/CVfolds/cv_folds_mass_70k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data\n",
    "\n",
    "X: Matrix of SNPs, y: Vector of pseudo-response (environmental effects removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-response stored under 'ID'. See dataloader, code from Steffi how it is processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used the predefined train/val/test sets from CreateCV. Setup is as follows:\n",
    "Each run has training data 80%, validation and test 10%.\n",
    "First run: testset = Fold 1 test, validation = Fold 2 test, trainset = Section 3-10 (i.e. Dataset - test Fold 1 - test Fold 2)\n",
    "...Then increment validation and test set once each run until final run when:\n",
    "testset = Fold 10 test, validation = Fold 1 test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#70k setup: Remove IID, 180k setup: remove FID. Else, everything is the same.\n",
    "# X is ringnrs + all SNPs\n",
    "X_CV = data.drop([\n",
    "            \"ID\",\n",
    "            \"mean_pheno\",\n",
    "            \"IID\",\n",
    "            \"MAT\",\n",
    "            \"PAT\",\n",
    "            \"SEX\",\n",
    "            \"PHENOTYPE\",\n",
    "            \"hatchisland\"\n",
    "        ], axis = 1)\n",
    "\n",
    "# Some of the SNPS have NA-values. Set to 0\n",
    "X_CV = X_CV.fillna(0)\n",
    "# Change from float to int64 for all columns not 'ringnr' (i.e. all SNPs)\n",
    "X_temp = X_CV.drop(['ringnr'], axis = 1)\n",
    "X_temp = X_temp.T.astype('int64').T\n",
    "X_temp.insert(0, 'ringnr', X_CV['ringnr'])\n",
    "X_CV = X_temp\n",
    "\n",
    "# y is ringnrs + pseudo phenotype\n",
    "y_CV = data[['ID', 'ringnr']]\n",
    "y_CV_mean = data[['mean_pheno', 'ringnr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find val and test indices by creating boolean mask\n",
    "test_idx = df[(df['Fold'] == 1) & (df['Set'] == 'test')]['ringnr'].values\n",
    "val_idx = df[(df['Fold'] == 2) & (df['Set'] == 'test')]['ringnr'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set equal to the intersect of training fold 1 and 2\n",
    "f1 = df[((df[\"Fold\"] == 1) & (df[\"Set\"] == 'train'))][\"ringnr\"]\n",
    "f2 = df[((df[\"Fold\"] == 2) & (df[\"Set\"] == 'train'))][\"ringnr\"]\n",
    "intersect = set(f1).intersection(set(f2))\n",
    "# Define training sets based on the intersection, then remove ringnrs\n",
    "X_train = X_CV[X_CV[\"ringnr\"].isin(intersect)].drop([\"ringnr\",], axis = 1)\n",
    "y_train = y_CV[y_CV[\"ringnr\"].isin(intersect)].drop([\"ringnr\",], axis = 1)\n",
    "y_mean_train = y_CV_mean[y_CV_mean[\"ringnr\"].isin(intersect)].drop([\"ringnr\",], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training-validation set, to be used when fitting model in testing. The train-val set is the whole training set excluding the testing-set (i.e. df training set Fold i_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_idx = df[(df['Fold'] == 1) & (df['Set'] == 'train')]['ringnr'].values # Same as test-idx (lower value)\n",
    "X_train_val = X_CV[X_CV[\"ringnr\"].isin(train_val_idx)].drop([\"ringnr\",],axis = 1)\n",
    "y_train_val = y_CV[y_CV['ringnr'].isin(train_val_idx)].drop([\"ringnr\",], axis = 1)\n",
    "y_mean_train_val = y_CV_mean[y_CV_mean['ringnr'].isin(train_val_idx)].drop([\"ringnr\",], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1529, 65238)\n",
      "(3467, 65239)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_CV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_CV[y_CV['ringnr'].isin(val_idx)][\"ID\"]\n",
    "y_test = y_CV[y_CV['ringnr'].isin(test_idx)][\"ID\"]\n",
    "\n",
    "y_mean_val = y_CV_mean[y_CV_mean['ringnr'].isin(val_idx)][\"mean_pheno\"]\n",
    "y_mean_test = y_CV_mean[y_CV_mean['ringnr'].isin(test_idx)][\"mean_pheno\"]\n",
    "\n",
    "X_val = X_CV[X_CV[\"ringnr\"].isin(val_idx)].drop([\"ringnr\",], axis = 1)\n",
    "X_test = X_CV[X_CV[\"ringnr\"].isin(test_idx)].drop([\"ringnr\",], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an objective function to be optimized. Used by optuna.\n",
    "\n",
    "See the following link for interpretation of hyperparameters:\n",
    "https://forecastegy.com/posts/xgboost-hyperparameter-tuning-with-optuna/\n",
    "\n",
    "n_estimators: Number of trees to be trained\n",
    "\n",
    "learning_rate: How much each tree contributes to the final prediction, and how 'weak'/'strong' the learner is\n",
    "\n",
    "max_depth: Maximum depth that a tree can grow to. Decides complexity of each tree in the model. Deeper tree: potentially capturing more complex patterns in the data, but risk overfitting training data.\n",
    "\n",
    "subsample: Proportion [0,1] of the dataset to be randomly selected for training each tree. Controls amount of data used for bulding each tree in the model. Less data may combat overfitting.\n",
    "\n",
    "colsample_bytree: Proportion [0,1] of features to be considered for each tree.\n",
    "\n",
    "min_child_weight: Sets minimum sum of instance weights that must be present in a child node in each tree. In regression, this just means the number of observations that must be present in each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Objective function to be optimized by package optuna.\n",
    "        Loss-function: MAE.\n",
    "        n_jobs: Amount of processors used. Computer-specific, needs to be \n",
    "                changed according to computer.\"\"\"\n",
    "    params = {\n",
    "        \"objective\": \"reg:absoluteerror\",\n",
    "        \"verbosity\": 0,\n",
    "        \"n_estimators\": 600, #trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 14),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 5, 25),\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    predictions = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, predictions)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the optimization, we create a study object and pass the objective function to the optimize method.\n",
    "\n",
    "The *direction* parameter specifies whether we want to minimize or maximize the objective function. The *n_trials* parameter defines the number of times the model will be trained with different hyperparameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study_full: Using full set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_full = optuna.create_study(direction='minimize')\n",
    "study_full.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the optimization is complete, we can display the best hyperparameters and the RMSE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.01686051134623099, 'max_depth': 7, 'subsample': 0.24635348736268714, 'colsample_bytree': 0.44407330245285953, 'min_child_weight': 19}\n",
      "Best MAE: 0.7832194356437325\n"
     ]
    }
   ],
   "source": [
    "print('Best hyperparameters:', study_full.best_params)\n",
    "print('Best MAE:', study_full.best_value)\n",
    "#Full run:\n",
    "#Best MAE: 0.8632183845614925, using 75/15/10%\n",
    "# With best params: {'learning_rate': 0.007761168942941743, 'max_depth': 7, 'subsample': 0.8431936111543968, 'colsample_bytree': 0.47520937351444037, 'min_child_weight': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the validation sets should be replaced by test sets. Keep in mind that the result is misleading as long as we are looking at correlation between predictions from training data and validation set. Validation set is used when optimizing hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(n_estimators = 600, learning_rate = 0.01686051134623099, max_depth = 7, subsample = 0.24635348736268714, colsample_bytree = 0.44407330245285953, min_child_weight = 19)\n",
    "model.fit(X_train_val, y_train_val, verbose = False)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2896237404549282"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = stats.pearsonr(y_test, predictions)[0]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some plots from the hyperparameteroptimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna.visualization.plot_optimization_history(study_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna.visualization.plot_slice(study_full, params=[\"learning_rate\",\"max_depth\", \"subsample\", \"colsample_bytree\", \"min_child_weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: If most of the best trials utilize a specific hyperparameter near the minimum or maximum value, consider expanding the search space for that hyperparameter.\n",
    "\n",
    "For example, if most of the best trials use learning_rate close to 0.001, you should probably restart the optimization with the search space trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna.visualization.plot_parallel_coordinate(study_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the following link for visualization of the hyperparamters in the hyperparameterspace: https://www.youtube.com/watch?v=t-INgABWULw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1667, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset\n",
    "X_train_val_sub, X_test_sub, y_train_val_sub, y_test_sub = train_test_split(X_subset, y, test_size=0.10, random_state=42)\n",
    "X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(X_train_val_sub, y_train_val_sub, test_size=0.1667, random_state=43)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
